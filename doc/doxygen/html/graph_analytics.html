<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9"/>
<title>GraphLab: Distributed Graph-Parallel API: Graph Analytics</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">GraphLab: Distributed Graph-Parallel API
   &#160;<span id="projectnumber">2.2</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('graph_analytics.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Graph Analytics </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>The graph analytics toolkit contains applications for performing graph analytics and extracting patterns from the graph structure.</p>
<p>The graph analytics toolkit contains applications for performing graph analytics and extracting patterns from the graph structure mostly from the Social Network Analysis Toolkit</p>
<p>The toolkit current contains:</p><ul>
<li><a class="el" href="graph_analytics.html#djikstra">Djisktra Algorithm Base</a></li>
<li><a class="el" href="graph_analytics.html#betweeness">Betweeness Algorithm</a></li>
<li><a class="el" href="graph_analytics.html#closeness">Closeness Algorithm</a></li>
<li><a class="el" href="graph_analytics.html#prestige">Prestge Algoritm</a></li>
</ul>
<p>All toolkits take any of the graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a> .</p>
<h1><a class="anchor" id="djikstra"></a>
"Djikstra Algorithm Base"</h1>
<p>The input format for the djikstra algorithm is</p>
<pre class="fragment">&lt;long node_id&gt; [&lt;long node_id&gt; &lt;float edge_value&gt;]*
</pre><p>The output format of the djikstra algorithm is</p>
<pre class="fragment">&lt;long node_id&gt; [&lt;long root_of_spanning_tree_id&gt; &lt;long next_node_higher_in_spanning_tree&gt;]*
</pre><p>roots do not list themselves</p>
<p>Run this command with:</p>
<pre class="fragment">mpiexec -n &lt;N machines&gt; --hostfile &lt;hostfile&gt; ./djikstra --graph &lt;graph location&gt; [--saveprefix &lt;prefix to attach to output&gt;] 
</pre><p>The output describes the spanning trees constructed starting at up to 3000 nodes</p>
<h2><a class="anchor" id="djikstra_imp"></a>
"Djikstra Algorithm Details"</h2>
<p>Djikstra starts with a randomly selected (~3000) set of nodes using a constant time and space random process. Each gather collects the earlier spanning tree members. The apply step calculates the best path, storing it. The scatter step notifies all nodes other than nodes that have notifid it to execute now. This process runs concurrently for each starting node.</p>
<p>Note: this algorithm does not preserve correctness of the total cost at each node, only relative stregth between node choices. Re-walk the spanning tree to calculate this (see below for examples).</p>
<h1><a class="anchor" id="betweeness"></a>
"Betweeness Algorithms"</h1>
<p>The input format for the betweeness algorithm is:</p>
<pre class="fragment">&lt;long node_id&gt; [&lt;long node_id&gt; &lt;float edge_value&gt;]*
</pre><p>The output format of the betweeness algorithm is</p>
<pre class="fragment">&lt;long node_id&gt; &lt;float betweeness score&gt;
</pre><p>Run this command with:</p>
<pre class="fragment">mpiexec -n &lt;N machines&gt; --hostfile &lt;hostfile&gt; ./betweeness --graph &lt;graph location&gt; [--saveprefix &lt;prefix to attach to output&gt;] 
</pre><p>The output estimates betweeness using ~3000 randomly selected spanning trees (typically +/-3% accuracy in the measure for each node.)</p>
<h2><a class="anchor" id="betweeness_imp"></a>
"Betweeness Algorithm Details"</h2>
<p>Djikstra sanning trees are calculated first, then the datsa structure is reset, then betweeness scores are calculated by walking the spanning trees from leaves to roots. Finally, the betweeness scores are collated from the various samples of spanning trees.</p>
<p>See djiksra_imp for details on how spanning trees are calculated.</p>
<p>The next step resets all the bookkeeping on the spanning trees and sets costs to zero.</p>
<p>Finally, the betweeness is calculated. All nodes are started at first, but only nodes without another spanning tree node pointing to it have a non-null execution.</p>
<p>The gather step checks if all nodes in the spanning tree pointing to it have been calculated yet, silently skipping if this is not true. Otherwise, the betweeness scores are collected.</p>
<p>The apply step sums the betweeness score for this node and spanning tree.</p>
<p>The scatter step signals the next higher node in this spanning tree that a new betweeness score is ready.</p>
<p>When the graph is saved, it outputs the sum of all betweeness scores across all calculated spanning trees and estimates the expected final betweeness score.</p>
<h1><a class="anchor" id="closeness"></a>
"Closeness Algorithm"</h1>
<p>The input format for the closeness algorithm is:</p>
<pre class="fragment">&lt;long node_id&gt; [&lt;long node_id&gt; &lt;float edge_value&gt;]*
</pre><p>The output format of the betweeness algorithm is</p>
<pre class="fragment">&lt;long node_id&gt; &lt;float closeness score&gt;
</pre><p>Run this command with:</p>
<pre class="fragment">mpiexec -n &lt;N machines&gt; --hostfile &lt;hostfile&gt; ./closeness --graph &lt;graph location&gt; [--saveprefix &lt;prefix to attach to output&gt;] 
</pre><p>The output estimates closeness using ~3000 randomly selected spanning trees (typically +/-3% accuracy in the measure for each node.)</p>
<h2><a class="anchor" id="closeness_imp"></a>
"Closeness Algorithm Details"</h2>
<p>Djikstra spanning trees are calculated first, then the datsa structure is reset, then closeness scores are calculated by walking the spanning trees from leaves to roots. Finally, the closeness scores are collated from the various samples of spanning trees.</p>
<p>See djiksra_imp for details on how spanning trees are calculated, except the link direction is reversed.</p>
<p>The next step resets all the bookkeeping on the spanning trees and sets costs to zero.</p>
<p>Finally, the closeness is calculated. The starting node set is reused, and the spanning trees are all walked simultaneously from root to leaves.</p>
<p>The gather step collects the parent closeness score.</p>
<p>The apply step combines the parent's closeness score with the edge value and stores it.</p>
<p>The scatter step signals al child nodes.</p>
<p>When the graph is saved, it outputs the sum of all closeness scores across all calculated spanning trees and estimates the expected final closeness score.</p>
<h1><a class="anchor" id="prestige"></a>
"Prestige Algorithm"</h1>
<p>The input format for the prestige algorithm is:</p>
<pre class="fragment">&lt;long node_id&gt; [&lt;long node_id&gt; &lt;float edge_value&gt;]*
</pre><p>The output format of the betweeness algorithm is</p>
<pre class="fragment">&lt;long node_id&gt; &lt;float prestige score&gt;
</pre><p>Run this command with:</p>
<pre class="fragment">mpiexec -n &lt;N machines&gt; --hostfile &lt;hostfile&gt; ./prestige --graph &lt;graph location&gt; [--saveprefix &lt;prefix to attach to output&gt;] 
</pre><p>The output estimates prestige using ~3000 randomly selected spanning trees (typically +/-3% accuracy in the measure for each node.)</p>
<h2><a class="anchor" id="prestige_imp"></a>
"Prestige Algorithm Details"</h2>
<p>Djikstra spanning trees are calculated first, then the datsa structure is reset, then prestige scores are calculated by walking the spanning trees from leaves to roots. Finally, the prestige scores are collated from the various samples of spanning trees.</p>
<p>See djiksra_imp for details on how spanning trees are calculated.</p>
<p>The next step resets all the bookkeeping on the spanning trees and sets costs to zero.</p>
<p>Finally, the prestige is calculated. The starting node set is reused, and the spanning trees are all walked simultaneously from root to leaves.</p>
<p>The gather step collects the parent prestige score.</p>
<p>The apply step combines the parent's prestige score with the edge value and stores it.</p>
<p>The scatter step signals al child nodes.</p>
<p>When the graph is saved, it outputs the sum of all prestige scores across all calculated spanning trees and estimates the expected final prestige score.</p>
<h1><a class="anchor" id="graph_analytics_pagerank"></a>
PageRank</h1>
<p>The PageRank program computes the pagerank of each vertex. See the <a href="http://en.wikipedia.org/wiki/PageRank">Wikipedia article</a> for details of the algorithm.</p>
<h2><a class="anchor" id="Input"></a>
Input</h2>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<pre class="fragment">&gt; ./pagerank --graph=[graph prefix] --format=[format] 
</pre><p>Alternatively, a synthetic power law graph of an arbitrary number of vertices can be generated using: </p><pre class="fragment">&gt; ./pagerank --powerlaw=[nvertices]
</pre><p> The resultant graph will have powerlaw out-degree, and nearly constant in-degree. The actual generation process draws vertex degree from a truncated power-law distribution with alpha=2.1. The distribution is truncated at maximum out-degree 100M to avoid allocating massive amounts of memory for creating the sampling distribution.</p>
<h2><a class="anchor" id="Computation"></a>
Type</h2>
<p>There are several modes of computation that are supported. All will eventually obtain the same solutions.</p>
<h3>Classical</h3>
<p>To get classical PageRank iterations, adding the option </p><pre class="fragment">&gt; --iterations=[N Iterations]
</pre><h3>Dynamic Synchronous (default)</h3>
<p>The dynamic synchronous computation only performs computation on vertices that have not yet converged to the desired tolerance. The default tolerance is 0.001. This can be modified by adding the option </p><pre class="fragment">&gt;  --tol=[tolerance]
</pre><h3>Dynamic Asynchronous</h3>
<p>The dynamic asynchronous computation only performs computation on vertices that have not yet converged to the desired tolerance. This uses the asynchronous engine. The default tolerance is 0.001. This can be modified by adding the option </p><pre class="fragment">&gt;  --tol=[tolerance]
</pre><dl class="section note"><dt>Note</dt><dd>This is known to be slow! PageRank does not benefit from the consistency guaranteed by the asynchronous engine. A new engine is in development with weaker consistency semantics, but sufficient for pagerank.</dd></dl>
<h2><a class="anchor" id="Output"></a>
Output</h2>
<p>To save the resultant pagerank of each vertex, include the option </p><pre class="fragment">&gt; --saveprefix=[output prefix]
</pre><p>Tne <code>output prefix</code> is where the output counts will be written. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">v_out_1_of_16
v_out_2_of_16
...
v_out_16_of_16
</pre><p>Each line in the output file contains two numbers: a Vertex ID, and the computed PageRank. Note that the output vector is NOT normalized, namely computed entries do not sum into one.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./pagerank ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Optional). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Optional). The format of the input graph </li>
<li><b>&ndash;powerlaw</b> (Optional. Default 0). If set, generates synthetic powerlaw graph with the specified number of vertices. </li>
<li><b>&ndash;saveprefix</b> (Optional. Default ""). If set, will write the output counts. </li>
<li><b>&ndash;tol</b> (Optional. Default=1E-3). Changes the convergence tolerance for the Dynamic computation modes. </li>
<li><b>&ndash;iterations</b> (Optional. Default 0). If set, runs classical PageRank iterations for the specified number of iterations. </li>
<li><b>-–graph_opts</b> (Optional, Default empty) Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>-–engine</b> (Optional, Default "synchronous") Sets the engine type. Must be either "synchronous" or "asynchronous" </li>
<li><b>-–engine</b> (Optional, Default "synchronous") Sets the engine options. Available options depend on the engine type. See <a class="el" href="classgraphlab_1_1async__consistent__engine.html" title="The asynchronous consistent engine executed vertex programs asynchronously and can ensure mutual excl...">graphlab::async_consistent_engine</a> and <a class="el" href="classgraphlab_1_1synchronous__engine.html" title="The synchronous engine executes all active vertex program synchronously in a sequence of super-step (...">graphlab::synchronous_engine</a> for details.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_kcore"></a>
KCore Decomposition</h1>
<p>This program iteratively finds the KCore of the network.</p>
<h2><a class="anchor" id="Input"></a>
Input</h2>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<pre class="fragment">&gt; ./kcore --graph=[graph prefix] --format=[format] 
</pre><p> Output may look like: </p><pre class="fragment">K=0:  #V = 875713   #E = 4322051
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 0
K=1:  #V = 875713   #E = 4322051
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 153407
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=2:  #V = 711870   #E = 4160100
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 108715
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=3:  #V = 581712   #E = 3915291
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 69907
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=4:  #V = 492655   #E = 3668104
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 52123
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=5:  #V = 424155   #E = 3416251
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 41269
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=6:  #V = 367361   #E = 3158776
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 33444
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=7:  #V = 319194   #E = 2902138
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 29201
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=8:  #V = 274457   #E = 2629033
......
</pre><p>To just get the informative lines: </p><pre class="fragment">&gt; ./kcore --graph=[graph prefix] --format=[format] &gt; k_out.txt
  ...
&gt; cat k_out.txt
Computes a k-core decomposition of a graph.

Number of vertices: 875713
Number of edges:    4322051
K=0:  #V = 875713   #E = 4322051
K=1:  #V = 875713   #E = 4322051
K=2:  #V = 711870   #E = 4160100
K=3:  #V = 581712   #E = 3915291
K=4:  #V = 492655   #E = 3668104
K=5:  #V = 424155   #E = 3416251
K=6:  #V = 367361   #E = 3158776
K=7:  #V = 319194   #E = 2902138
K=8:  #V = 274457   #E = 2629033
K=9:  #V = 231775   #E = 2335154
K=10:  #V = 193406   #E = 2040738
K=11:  #V = 159020   #E = 1753273
K=12:  #V = 131362   #E = 1500517
K=13:  #V = 106572   #E = 1256952
K=14:  #V = 86302   #E = 1047053
K=15:  #V = 68409   #E = 849471
K=16:  #V = 53459   #E = 676076
K=17:  #V = 40488   #E = 519077
...
</pre><p>The program can also save a copy of the graph at each stage by adding an option. </p><pre class="fragment">&gt; --savecores=[prefix]
</pre><p>The resultant graphs will be saved with prefixes [prefix].K For instance if prefix is <code>out</code>, The 0-Core graph may be saved in </p><pre class="fragment">out.0.1_of_4
out.0.2_of_4
out.0.3_of_4
out.0.4_of_4
</pre><p>The 5-Core graph will be saved in </p><pre class="fragment">out.5.1_of_4
out.5.2_of_4
out.5.3_of_4
out.5.4_of_4
</pre><p>and so on.</p>
<p>The range of k-Core graphs to compute can be controlled by the <code>kmin</code> and the <code>kmax</code> option described below.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./kcore....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>&ndash;savecores</b> (Optional. Default ""). The target prefix to save the resultant K-core graphs. </li>
<li><b>&ndash;kmin</b> (Optional. Default 0). Only output result for the K-core graph starting at K=kmin </li>
<li><b>&ndash;kmax</b> (Optional. Default Inf). Only output result for the K-core graph up to K=kmax</li>
</ul>
<h1><a class="anchor" id="graph_analytics_triangle_coloring"></a>
Graph Coloring</h1>
<p>The graph coloring program implements a really simple graph coloring procedure: each vertex reads the colors of its neighbors and takes on the smallest possible color which does not conflict with its neighbors.</p>
<p>The procedure necessarily uses the asynchronous engine (it will never converge with the synchronous engine).</p>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>. It is important that the input be "cleaned" and that reverse edges are removed: i.e. if edge 1&ndash;&gt;5 exists, edge 5&ndash;&gt;1 should not exist. (The program will run without these edge removed. But numbers may be erroneous).</p>
<p>To color a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./simple_coloring --graph=[graph prefix] --format=[format] --output=[output prefix]
</pre><p> Output looks like: </p><pre class="fragment">Number of vertices: 875713
Number of edges:    5105039
Coloring...
Completed Tasks: 875713
Issued Tasks: 875713
Blocked Issues: 0
------------------
Joined Tasks: 0
Colored in 42.3684 seconds
Metrics server stopping.
</pre><p>Observe that the number of Completed Tasks is identical to the number of vertices. This is a result of the consistency model which ensures that the entire vertex update is peformed "atomically".</p>
<p>Tne <code>output prefix</code> is where the output counts will be written. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">v_out_1_of_16
v_out_2_of_16
...
v_out_16_of_16
</pre><p>Each line in the output file contains two numbers: a Vertex ID, and the number color of the vertex.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./simple_coloring ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>–-graph_opts</b> (Optional, Default empty) Any additional graph options. See &ndash;graph_help a list of options. </li>
<li><b>–-engine_opts</b> (Optional, Default empty) Any additional engine options. See &ndash;engine_help a list of options.</li>
</ul>
<p>A particularly relevant option is </p><pre class="fragment">--engine_opts="factorized=true"
</pre><p>This uses a weaker consistency setting which only guarantees that individual "gather/apply/scatter" operations are atomic, but does not guarantee atomicity of the entire update. As a result, this may require more updates to complete, but could in practice run significantly faster.</p>
<h1><a class="anchor" id="graph_analytics_connected_component"></a>
Connected Component</h1>
<p>The connected component program can find all connected components in a graph, and can also count the number of vertices (size) of each connected component.</p>
<p>The input to the system is a graph in any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<p>To find connected components in a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./connected_component --graph=[graph prefix] --format=[format]
</pre><p>Here is a toy example, graph with 6 nodes and 5 edges: </p><pre class="fragment"># example graph
# vertices: 6 edges: 5
1 2
2 3
4 5
4 6
5 6
</pre><p>Assume file name is toy_graph, the command used for running connected compnents is </p><pre class="fragment">&gt; ./connected_component --graph=toy_graph --format=tsv --saveprefix=out
</pre><p>When you set <code>&ndash;saveprefix=output_prefix</code>, the pairs of a Vertex ID and a Component ID will be written to a sequence of files with prefix <code>output_prefix</code>. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">out_1_of_4
out_2_of_4
out_3_of_4
out_4_of_4
</pre><p>Let's examine the output. The first column is the node id, while the second column is it's assigned component number (which is also the lowest node id in this component). In our case: </p><pre class="fragment">1,1
2,1
3,1
4,4
5,4
6,4
</pre><p>There are two components. The first compoent is 1,2,3 and the second component is 4,5,6</p>
<p>Note that this program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./connected_component ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;saveprefix</b> (Optional). If set, pairs of a Vertex ID and a Component ID will be saved to a sequence of files with the given prefix. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2). The number of processors that will be used for computation. </li>
<li><b>&ndash;graph_opts</b> (Optional, Default empty). Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options.</li>
</ul>
<p>connected_components_stats is a helper utility, which computes histogram of component sizes.</p>
<p>Using our toy example </p><pre class="fragment">&gt; ./connected_component_stats --graph=out
Connected Component

INFO:     mpi_tools.hpp(init:63): MPI Support was not compiled.
INFO:     dc.cpp(init:573): Cluster of 1 instances created.
INFO:     distributed_graph.hpp(set_ingress_method:3200): Automatically determine ingress method: grid
Loading graph in format: adj
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_1_of_4
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_2_of_4
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_3_of_4
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_4_of_4
INFO:     distributed_ingress_base.hpp(finalize:185): Finalizing Graph...
INFO:     distributed_ingress_base.hpp(exchange_global_info:519): Graph info:
   nverts: 2
   nedges: 0
   nreplicas: 2
   replication factor: 1
Complete Finalization in 0.001965
graph calculation time is 2.4e-05 sec
RESULT:
size  count
3 2
</pre><p>As expected, there are two components of size 3.</p>
<h1><a class="anchor" id="graph_analytics_approximate_diameter"></a>
Approximate Diameter</h1>
<p>The approximate diameter program can estimate a diameter of a graph. The implemented algorithm is based on the work,</p>
<p>U Kang, Charalampos Tsourakakis, Ana Paula Appel, Christos Faloutsos and Jure Leskovec, HADI: Fast Diameter Estimation and Mining in Massive Graphs with Hadoop (2008).</p>
<p>The input to the system is a graph in any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<p>To compute an approximate diameter of a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./approximate_diameter --graph=[graph prefix] --format=[format]
</pre><p> Output looks like: </p><pre class="fragment">Approximate graph diameter
INFO:     synchronous_engine.hpp(start:1263): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1312):   Active vertices: 1271950
INFO:     synchronous_engine.hpp(start:1361):    Running Aggregators
1-th hop: 12895307 vertex pairs are reached
INFO:     synchronous_engine.hpp(start:1263): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1312):   Active vertices: 1271950
INFO:     synchronous_engine.hpp(start:1361):    Running Aggregators
2-th hop: 319726269 vertex pairs are reached
INFO:     synchronous_engine.hpp(start:1263): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1312):   Active vertices: 1271950
INFO:     synchronous_engine.hpp(start:1361):    Running Aggregators
3-th hop: 319769151 vertex pairs are reached
converge
graph calculation time is 40 sec
approximate diameter is 2
</pre><p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./approximate_diameter ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;tol</b> (Optional. Default=1E-4). Changes the convergence tolerance for the number of reached vertex pairs at each hop. </li>
<li><b>&ndash;use-sketch</b> (Optional. Default=1). If true, will use Flajolet &amp; Martin bitmask to approximately count numbers of reached vertex pairs, and will require a smaller memory. If false, will count exact numbers of reached vertex pairs. But this will need a huge memory and be slow. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2). The number of processors that will be used for computation. </li>
<li><b>&ndash;graph_opts</b> (Optional, Default empty). Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_partitioning"></a>
Graph Partitioning</h1>
<p>This program can partition a graph by using normalized cut.</p>
<p>The input to the system is a graph in any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>. You can also give weights to edges with the <code>weight</code> format. For instance in this <code>weight</code> format file, there are 5 edges:</p>
<pre class="fragment">1 2 4.0
2 3 1.0
3 4 5.0
4 5 2.0
5 3 3.0
</pre><p>To partition a graph, the minimal set of options required are:</p>
<pre class="fragment">&gt; ./partitioning --graph=[graph prefix] --format=[format]
</pre><p>This program uses svd in Graphlab Collaborative Filtering Toolkit and kmeans in Graphlab Clustering Toolkit. The paths to the directories are specified by <code>&ndash;svd-dir</code> and <code>&ndash;kmeans-dir</code>, respectively.</p>
<p>The program will create some intermediate files. The final partitioning result is written in files named <code>[graph prefix].result</code> with suffix, for example <code>[graph prefix].result_1_of_4</code>. The partitioning result data consists of two columns: one for the ids and the other for the assigned partitions. For instance:</p>
<pre class="fragment">1 0
2 0
3 1
4 1
5 1
</pre><p><b>NOTE:</b> To run this program in a distributed setting, you must use the "mpi-args" option, not like other graphlab toolkits. The graph partitioning calls other graphlab programs. When "--mpi-args" is set, these graphlab programs are called with "mpiexec" and the string written after the "mpi-args" option. For example, if you set &ndash;mpi-args="-n 4 --hostfile host", the program calls the other graphlab programs with "mpiexec -n 4 --hostfile host".</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph. If "weight" is set, the program will read the data file where each line holds [id1] [id2] [weight]. </li>
<li><b>&ndash;partitions</b> (Optional. Default 2). The number of partitions </li>
<li><b>&ndash;svd-dir</b> (Optional. Default ../collaborative_filtering/). Path to the directory where Graphlab svd is located </li>
<li><b>&ndash;kmeans-dir</b> (Optional. Default ../clustering/). Path to the directory where Graphlab kmeans is located </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2). The number of processors that will be used for computation. </li>
<li><b>&ndash;graph_opts</b> (Optional, Default empty). Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options. </li>
<li><b>&ndash;mpi-args</b> (Optional, Default empty). If set, will execute mipexec with the given string.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_total_subgraph_centrality"></a>
"Total Subgraph Centrality"</h1>
<p>Total subgraph centrality was implemented by Jacob Kesinger, see additional details in his <a href="http://jacobkesinger.tumblr.com/post/64338572799/total-subgraph-centrality">blog post</a>. Total Subgraph Communicability is a new centrality measure due to Benzi&amp;Klymco [1]. For a directed graph with adjacenty matrix A,</p>
<pre class="fragment">TSC_i = sum_j exp(A)_{ij} = (exp(A)*1)_i.
</pre><p>This code calculates the TSC using an Arnoldi iteration on the Krylov subspace {b, Ab,A*Ab, A*A*Ab, ...} due to Saad[1], and using the new warp engine from Graphlab 2.2 (without which this would have been, at best, very challenging).</p>
<p>Small components of large graphs will have bogus answers due to floating point issues. To find the exact TSC for a particular node i, run with "--column i" to find exp(A)*e_i; you will have to sum the resulting output yourself, however.</p>
<p>SAMPLE INPUT: </p><pre class="fragment">0 1
1 2
1 3
2 4
3 4
1 0
2 1
3 1
4 2
4 3
</pre><p>OUTPUT: </p><pre class="fragment">0 5.17784
1 10.3319
2 8.49789
3 8.49789
4 7.96807
</pre><p>You can verify this in python as: </p><pre class="fragment">import scipy
import scipy.linalg
A = scipy.array([[0,1,0,0,0],[1,0,1,1,0],[0,1,0,0,1],[0,1,0,0,1],[0,0,1,1,0]])
scipy.linalg.expm2(A).sum(axis=1)
</pre><p>[1]: Benzi, Michele, and Christine Klymko. Total Communicability as a Centrality Measure. ArXiv e-print, February 27, 2013. <a href="http://arxiv.org/abs/1302.6770">arxiv</a></p>
<p>[2]: Saad, Yousef. “Analysis of Some Krylov Subspace Approximations to the Matrix Exponential Operator.” SIAM Journal on Numerical Analysis 29, no. 1 (1992): 209–228.</p>
<p>The graph analytics toolkit contains applications for performing graph analytics and extracting patterns from the graph structure.</p>
<p>The toolkit current contains:</p><ul>
<li><a class="el" href="graph_analytics.html#graph_analytics_format_conversion">Graph Format Conversion</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_triangle_undirected">Triangle Counting (undirected)</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_triangle_directed">Triangle Counting (directed)</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_pagerank">PageRank</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_kcore">KCore Decomposition</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_connected_component">Connected Component</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_approximate_diameter">Approximate Diameter</a></li>
<li><a class="el" href="graph_analytics.html#graph_analytics_partitioning">Graph Partitioning</a></li>
<li>Graph Coloring</li>
<li><a class="el" href="graph_analytics.html#graph_analytics_total_subgraph_centrality">Total Subgraph Centrality</a></li>
</ul>
<p>All toolkits take any of the graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a> .</p>
<h1><a class="anchor" id="graph_analytics_format_conversion"></a>
Format Conversion</h1>
<p>This is primarily a utility program, providing conversion between any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<p>To run: </p><pre class="fragment">&gt; ./format_convert --ingraph=[input graph location] --informat=[input format type]
                   --outgraph=[output graph location] --outformat[output format type]
</pre><p>The output is by default gzip compressed. To disable, add the option, </p><pre class="fragment"> --outgzip=0
</pre><p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./format_convert ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h1><a class="anchor" id="graph_analytics_triangle_undirected"></a>
Undirected Triangle Counting</h1>
<p>The undirected triangle counting program can count the total number of triangles in a graph, and can also, with little more time, count the number of triangles passing through each vertex in the graph.</p>
<p>It implements the edge-iterator algorithm described in</p>
<p>T. Schank. Algorithmic Aspects of Triangle-Based Network Analysis. Phd in computer science, University Karlsruhe, 2007.</p>
<p>with several optimizations.</p>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>. It is important that the input be "cleaned" and that reverse edges are removed: i.e. if edge 1&ndash;&gt;5 exists, edge 5&ndash;&gt;1 should not exist. (The program will run without these edge removed. But numbers may be erroneous).</p>
<p>To count the total number of triangles in a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./undirected_triangle_count --graph=[graph prefix] --format=[format]
</pre><p> Output looks like: </p><pre class="fragment">Number of vertices: 875713
Number of edges:    4322051
Counting Triangles...
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 875713
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
Counted in 1.16463 seconds
13391903 Triangles
</pre><p>To count the number of triangles on each vertex, the minimal set of options are:</p>
<pre class="fragment">&gt; ./undirected_triangle_count --graph=[graph prefix] --format=[format] --per_vertex=[output prefix]
</pre><p>Tne <code>output prefix</code> is where the output counts will be written. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">v_out_1_of_16
v_out_2_of_16
...
v_out_16_of_16
</pre><p>Each line in the output file contains two numbers: a Vertex ID, and the number of triangles intersecting the vertex.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./undirected_triangle_count ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;per_vertex</b> (Optional. Default ""). If set, will write the output counts. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>&ndash;ht</b> (Optional. Default 64) The implementation uses a mix of vectors and hash sets to optimize set intersection computation. This parameter sets the capacity limit below which, vectors are used, and above which, hash sets are used. </li>
<li><b>–-graph_opts</b> (Optional, Default empty) Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_triangle_directed"></a>
Directed Triangle Counting</h1>
<p>The directed triangle counting program counts the total number of directed triangles in a graph of each type, and can also output the number of triangles of each type passing through each vertex in the graph.</p>
<p>We show the 4 possible types of triangles here: In each case, the vertex being evaluated is the green vertex labeled "A". A dotted edge means that the direction of the edge do not matter.</p>
<table class="doxtable">
<tr>
<th>Triangle Name </th><th>Triangle Pattern  </th></tr>
<tr>
<td>In Triangle </td><td><div class="image">
<img src="in_triangle.gif" alt="in_triangle.gif"/>
</div>
 </td></tr>
<tr>
<td>Out Triangle </td><td><div class="image">
<img src="out_triangle.gif" alt="out_triangle.gif"/>
</div>
 </td></tr>
<tr>
<td>Through Triangle </td><td><div class="image">
<img src="through_triangle.gif" alt="through_triangle.gif"/>
</div>
 </td></tr>
<tr>
<td>Cycle Triangle </td><td><div class="image">
<img src="cycle_triangle.gif" alt="cycle_triangle.gif"/>
</div>
 </td></tr>
</table>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<p>To count the total number of triangles in a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./directed_triangle_count --graph=[graph prefix] --format=[format]
</pre><p> Output looks like this: </p><pre class="fragment">Number of vertices: 875713
Number of edges:    5105039
Counting Triangles...
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 875713
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
Counted in 1.962 seconds
Collecting results ... 
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 875713
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
28198954 In triangles
28198954 Out triangles
28198954 Through triangles
11669313 Cycle triangles
</pre><p> Observe that the number of In, Out and Through triangles are identical. This is because every In-triangle necessarily forms one Out and one Through triangle, (and similarly for the rest). Also the number of Cycle Triangles must be divisible by 3 since every cycle is counted 3 times, once on each vertex in the cycle.</p>
<p>To count the number of triangles on each vertex, the minimal set of options are:</p>
<pre class="fragment">&gt; ./directed_triangle_count --graph=[graph prefix] --format=[format] --per_vertex=[output prefix]
</pre><p>Tne <code>output prefix</code> is where the output counts will be written. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">v_out_1_of_16
v_out_2_of_16
...
v_out_16_of_16
</pre><p>Each line in the output file has the following format: </p><pre class="fragment">[vid]  [in triangles]  [out triangles]   [through triangles]  [cycle_triangles] [#out edges] [#in edges]
</pre><p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./directed_triangle_count ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;per_vertex</b> (Optional. Default ""). If set, will write the output counts. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>&ndash;ht</b> (Optional. Default 64) The implementation uses a mix of vectors and hash sets to optimize set intersection computation. This parameter sets the capacity limit below which, vectors are used, and above which, hash sets are used. </li>
<li><b>-–graph_opts</b> (Optional, Default empty) Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_pagerank"></a>
PageRank</h1>
<p>The PageRank program computes the pagerank of each vertex. See the <a href="http://en.wikipedia.org/wiki/PageRank">Wikipedia article</a> for details of the algorithm.</p>
<h2><a class="anchor" id="Input"></a>
Input</h2>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<pre class="fragment">&gt; ./pagerank --graph=[graph prefix] --format=[format] 
</pre><p>Alternatively, a synthetic power law graph of an arbitrary number of vertices can be generated using: </p><pre class="fragment">&gt; ./pagerank --powerlaw=[nvertices]
</pre><p> The resultant graph will have powerlaw out-degree, and nearly constant in-degree. The actual generation process draws vertex degree from a truncated power-law distribution with alpha=2.1. The distribution is truncated at maximum out-degree 100M to avoid allocating massive amounts of memory for creating the sampling distribution.</p>
<h2><a class="anchor" id="Computation"></a>
Type</h2>
<p>There are several modes of computation that are supported. All will eventually obtain the same solutions.</p>
<h3>Classical</h3>
<p>To get classical PageRank iterations, adding the option </p><pre class="fragment">&gt; --iterations=[N Iterations]
</pre><h3>Dynamic Synchronous (default)</h3>
<p>The dynamic synchronous computation only performs computation on vertices that have not yet converged to the desired tolerance. The default tolerance is 0.001. This can be modified by adding the option </p><pre class="fragment">&gt;  --tol=[tolerance]
</pre><h3>Dynamic Asynchronous</h3>
<p>The dynamic asynchronous computation only performs computation on vertices that have not yet converged to the desired tolerance. This uses the asynchronous engine. The default tolerance is 0.001. This can be modified by adding the option </p><pre class="fragment">&gt;  --tol=[tolerance]
</pre><dl class="section note"><dt>Note</dt><dd>This is known to be slow! PageRank does not benefit from the consistency guaranteed by the asynchronous engine. A new engine is in development with weaker consistency semantics, but sufficient for pagerank.</dd></dl>
<h2><a class="anchor" id="Output"></a>
Output</h2>
<p>To save the resultant pagerank of each vertex, include the option </p><pre class="fragment">&gt; --saveprefix=[output prefix]
</pre><p>Tne <code>output prefix</code> is where the output counts will be written. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">v_out_1_of_16
v_out_2_of_16
...
v_out_16_of_16
</pre><p>Each line in the output file contains two numbers: a Vertex ID, and the computed PageRank. Note that the output vector is NOT normalized, namely computed entries do not sum into one.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./pagerank ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Optional). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Optional). The format of the input graph </li>
<li><b>&ndash;powerlaw</b> (Optional. Default 0). If set, generates synthetic powerlaw graph with the specified number of vertices. </li>
<li><b>&ndash;saveprefix</b> (Optional. Default ""). If set, will write the output counts. </li>
<li><b>&ndash;tol</b> (Optional. Default=1E-3). Changes the convergence tolerance for the Dynamic computation modes. </li>
<li><b>&ndash;iterations</b> (Optional. Default 0). If set, runs classical PageRank iterations for the specified number of iterations. </li>
<li><b>-–graph_opts</b> (Optional, Default empty) Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>-–engine</b> (Optional, Default "synchronous") Sets the engine type. Must be either "synchronous" or "asynchronous" </li>
<li><b>-–engine</b> (Optional, Default "synchronous") Sets the engine options. Available options depend on the engine type. See <a class="el" href="classgraphlab_1_1async__consistent__engine.html" title="The asynchronous consistent engine executed vertex programs asynchronously and can ensure mutual excl...">graphlab::async_consistent_engine</a> and <a class="el" href="classgraphlab_1_1synchronous__engine.html" title="The synchronous engine executes all active vertex program synchronously in a sequence of super-step (...">graphlab::synchronous_engine</a> for details.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_kcore"></a>
KCore Decomposition</h1>
<p>This program iteratively finds the KCore of the network.</p>
<h2><a class="anchor" id="Input"></a>
Input</h2>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<pre class="fragment">&gt; ./kcore --graph=[graph prefix] --format=[format] 
</pre><p> Output may look like: </p><pre class="fragment">K=0:  #V = 875713   #E = 4322051
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 0
K=1:  #V = 875713   #E = 4322051
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 153407
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=2:  #V = 711870   #E = 4160100
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 108715
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=3:  #V = 581712   #E = 3915291
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 69907
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=4:  #V = 492655   #E = 3668104
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 52123
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=5:  #V = 424155   #E = 3416251
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 41269
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=6:  #V = 367361   #E = 3158776
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 33444
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=7:  #V = 319194   #E = 2902138
INFO:     synchronous_engine.hpp(start:1213): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1257):   Active vertices: 29201
INFO:     synchronous_engine.hpp(start:1307):    Running Aggregators
K=8:  #V = 274457   #E = 2629033
......
</pre><p>To just get the informative lines: </p><pre class="fragment">&gt; ./kcore --graph=[graph prefix] --format=[format] &gt; k_out.txt
  ...
&gt; cat k_out.txt
Computes a k-core decomposition of a graph.

Number of vertices: 875713
Number of edges:    4322051
K=0:  #V = 875713   #E = 4322051
K=1:  #V = 875713   #E = 4322051
K=2:  #V = 711870   #E = 4160100
K=3:  #V = 581712   #E = 3915291
K=4:  #V = 492655   #E = 3668104
K=5:  #V = 424155   #E = 3416251
K=6:  #V = 367361   #E = 3158776
K=7:  #V = 319194   #E = 2902138
K=8:  #V = 274457   #E = 2629033
K=9:  #V = 231775   #E = 2335154
K=10:  #V = 193406   #E = 2040738
K=11:  #V = 159020   #E = 1753273
K=12:  #V = 131362   #E = 1500517
K=13:  #V = 106572   #E = 1256952
K=14:  #V = 86302   #E = 1047053
K=15:  #V = 68409   #E = 849471
K=16:  #V = 53459   #E = 676076
K=17:  #V = 40488   #E = 519077
...
</pre><p>The program can also save a copy of the graph at each stage by adding an option. </p><pre class="fragment">&gt; --savecores=[prefix]
</pre><p>The resultant graphs will be saved with prefixes [prefix].K For instance if prefix is <code>out</code>, The 0-Core graph may be saved in </p><pre class="fragment">out.0.1_of_4
out.0.2_of_4
out.0.3_of_4
out.0.4_of_4
</pre><p>The 5-Core graph will be saved in </p><pre class="fragment">out.5.1_of_4
out.5.2_of_4
out.5.3_of_4
out.5.4_of_4
</pre><p>and so on.</p>
<p>The range of k-Core graphs to compute can be controlled by the <code>kmin</code> and the <code>kmax</code> option described below.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./kcore....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>&ndash;savecores</b> (Optional. Default ""). The target prefix to save the resultant K-core graphs. </li>
<li><b>&ndash;kmin</b> (Optional. Default 0). Only output result for the K-core graph starting at K=kmin </li>
<li><b>&ndash;kmax</b> (Optional. Default Inf). Only output result for the K-core graph up to K=kmax</li>
</ul>
<h1><a class="anchor" id="graph_analytics_triangle_coloring"></a>
Graph Coloring</h1>
<p>The graph coloring program implements a really simple graph coloring procedure: each vertex reads the colors of its neighbors and takes on the smallest possible color which does not conflict with its neighbors.</p>
<p>The procedure necessarily uses the asynchronous engine (it will never converge with the synchronous engine).</p>
<p>The input to the system is a graph in any of the Portable graph format described in <a class="el" href="graph_formats.html">Graph File Formats</a>. It is important that the input be "cleaned" and that reverse edges are removed: i.e. if edge 1&ndash;&gt;5 exists, edge 5&ndash;&gt;1 should not exist. (The program will run without these edge removed. But numbers may be erroneous).</p>
<p>To color a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./simple_coloring --graph=[graph prefix] --format=[format] --output=[output prefix]
</pre><p> Output looks like: </p><pre class="fragment">Number of vertices: 875713
Number of edges:    5105039
Coloring...
Completed Tasks: 875713
Issued Tasks: 875713
Blocked Issues: 0
------------------
Joined Tasks: 0
Colored in 42.3684 seconds
Metrics server stopping.
</pre><p>Observe that the number of Completed Tasks is identical to the number of vertices. This is a result of the consistency model which ensures that the entire vertex update is peformed "atomically".</p>
<p>Tne <code>output prefix</code> is where the output counts will be written. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">v_out_1_of_16
v_out_2_of_16
...
v_out_16_of_16
</pre><p>Each line in the output file contains two numbers: a Vertex ID, and the number color of the vertex.</p>
<p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./simple_coloring ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2) The number of processors that will be used for computation. </li>
<li><b>–-graph_opts</b> (Optional, Default empty) Any additional graph options. See &ndash;graph_help a list of options. </li>
<li><b>–-engine_opts</b> (Optional, Default empty) Any additional engine options. See &ndash;engine_help a list of options.</li>
</ul>
<p>A particularly relevant option is </p><pre class="fragment">--engine_opts="factorized=true"
</pre><p>This uses a weaker consistency setting which only guarantees that individual "gather/apply/scatter" operations are atomic, but does not guarantee atomicity of the entire update. As a result, this may require more updates to complete, but could in practice run significantly faster.</p>
<h1><a class="anchor" id="graph_analytics_connected_component"></a>
Connected Component</h1>
<p>The connected component program can find all connected components in a graph, and can also count the number of vertices (size) of each connected component.</p>
<p>The input to the system is a graph in any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<p>To find connected components in a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./connected_component --graph=[graph prefix] --format=[format]
</pre><p>Here is a toy example, graph with 6 nodes and 5 edges: </p><pre class="fragment"># example graph
# vertices: 6 edges: 5
1 2
2 3
4 5
4 6
5 6
</pre><p>Assume file name is toy_graph, the command used for running connected compnents is </p><pre class="fragment">&gt; ./connected_component --graph=toy_graph --format=tsv --saveprefix=out
</pre><p>When you set <code>&ndash;saveprefix=output_prefix</code>, the pairs of a Vertex ID and a Component ID will be written to a sequence of files with prefix <code>output_prefix</code>. This may be located on HDFS. For instance, if the <code>output_prefix</code> is <code>"v_out"</code>, the output files will be written to:</p>
<pre class="fragment">out_1_of_4
out_2_of_4
out_3_of_4
out_4_of_4
</pre><p>Let's examine the output. The first column is the node id, while the second column is it's assigned component number (which is also the lowest node id in this component). In our case: </p><pre class="fragment">1,1
2,1
3,1
4,4
5,4
6,4
</pre><p>There are two components. The first compoent is 1,2,3 and the second component is 4,5,6</p>
<p>Note that this program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./connected_component ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;saveprefix</b> (Optional). If set, pairs of a Vertex ID and a Component ID will be saved to a sequence of files with the given prefix. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2). The number of processors that will be used for computation. </li>
<li><b>&ndash;graph_opts</b> (Optional, Default empty). Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options.</li>
</ul>
<p>connected_components_stats is a helper utility, which computes histogram of component sizes.</p>
<p>Using our toy example </p><pre class="fragment">&gt; ./connected_component_stats --graph=out
Connected Component

INFO:     mpi_tools.hpp(init:63): MPI Support was not compiled.
INFO:     dc.cpp(init:573): Cluster of 1 instances created.
INFO:     distributed_graph.hpp(set_ingress_method:3200): Automatically determine ingress method: grid
Loading graph in format: adj
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_1_of_4
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_2_of_4
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_3_of_4
INFO:     distributed_graph.hpp(load_from_posixfs:2189): Loading graph from file: ./out_4_of_4
INFO:     distributed_ingress_base.hpp(finalize:185): Finalizing Graph...
INFO:     distributed_ingress_base.hpp(exchange_global_info:519): Graph info:
   nverts: 2
   nedges: 0
   nreplicas: 2
   replication factor: 1
Complete Finalization in 0.001965
graph calculation time is 2.4e-05 sec
RESULT:
size  count
3 2
</pre><p>As expected, there are two components of size 3.</p>
<h1><a class="anchor" id="graph_analytics_approximate_diameter"></a>
Approximate Diameter</h1>
<p>The approximate diameter program can estimate a diameter of a graph. The implemented algorithm is based on the work,</p>
<p>U Kang, Charalampos Tsourakakis, Ana Paula Appel, Christos Faloutsos and Jure Leskovec, HADI: Fast Diameter Estimation and Mining in Massive Graphs with Hadoop (2008).</p>
<p>The input to the system is a graph in any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>.</p>
<p>To compute an approximate diameter of a graph, the minimal set of options required are: </p><pre class="fragment">&gt; ./approximate_diameter --graph=[graph prefix] --format=[format]
</pre><p> Output looks like: </p><pre class="fragment">Approximate graph diameter
INFO:     synchronous_engine.hpp(start:1263): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1312):   Active vertices: 1271950
INFO:     synchronous_engine.hpp(start:1361):    Running Aggregators
1-th hop: 12895307 vertex pairs are reached
INFO:     synchronous_engine.hpp(start:1263): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1312):   Active vertices: 1271950
INFO:     synchronous_engine.hpp(start:1361):    Running Aggregators
2-th hop: 319726269 vertex pairs are reached
INFO:     synchronous_engine.hpp(start:1263): 0: Starting iteration: 0
INFO:     synchronous_engine.hpp(start:1312):   Active vertices: 1271950
INFO:     synchronous_engine.hpp(start:1361):    Running Aggregators
3-th hop: 319769151 vertex pairs are reached
converge
graph calculation time is 40 sec
approximate diameter is 2
</pre><p>This program can also run distributed by using </p><pre class="fragment">&gt; mpiexec -n [N machines] --hostfile [host file] ./approximate_diameter ....
</pre><p> See your MPI documentation for details on how to launch this job. All machines must have access to the input graph location and the output graph location. Graphs may be on HDFS. If you have problems loading HDFS files, see the <a class="el" href="FAQ.html">FAQ</a>.</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph </li>
<li><b>&ndash;tol</b> (Optional. Default=1E-4). Changes the convergence tolerance for the number of reached vertex pairs at each hop. </li>
<li><b>&ndash;use-sketch</b> (Optional. Default=1). If true, will use Flajolet &amp; Martin bitmask to approximately count numbers of reached vertex pairs, and will require a smaller memory. If false, will count exact numbers of reached vertex pairs. But this will need a huge memory and be slow. </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2). The number of processors that will be used for computation. </li>
<li><b>&ndash;graph_opts</b> (Optional, Default empty). Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_partitioning"></a>
Graph Partitioning</h1>
<p>This program can partition a graph by using normalized cut.</p>
<p>The input to the system is a graph in any of the Portable Graph formats described in <a class="el" href="graph_formats.html">Graph File Formats</a>. You can also give weights to edges with the <code>weight</code> format. For instance in this <code>weight</code> format file, there are 5 edges:</p>
<pre class="fragment">1 2 4.0
2 3 1.0
3 4 5.0
4 5 2.0
5 3 3.0
</pre><p>To partition a graph, the minimal set of options required are:</p>
<pre class="fragment">&gt; ./partitioning --graph=[graph prefix] --format=[format]
</pre><p>This program uses svd in Graphlab Collaborative Filtering Toolkit and kmeans in Graphlab Clustering Toolkit. The paths to the directories are specified by <code>&ndash;svd-dir</code> and <code>&ndash;kmeans-dir</code>, respectively.</p>
<p>The program will create some intermediate files. The final partitioning result is written in files named <code>[graph prefix].result</code> with suffix, for example <code>[graph prefix].result_1_of_4</code>. The partitioning result data consists of two columns: one for the ids and the other for the assigned partitions. For instance:</p>
<pre class="fragment">1 0
2 0
3 1
4 1
5 1
</pre><p><b>NOTE:</b> To run this program in a distributed setting, you must use the "mpi-args" option, not like other graphlab toolkits. The graph partitioning calls other graphlab programs. When "--mpi-args" is set, these graphlab programs are called with "mpiexec" and the string written after the "mpi-args" option. For example, if you set &ndash;mpi-args="-n 4 --hostfile host", the program calls the other graphlab programs with "mpiexec -n 4 --hostfile host".</p>
<h2><a class="anchor" id="Options"></a>
Options</h2>
<p>Relevant options are: </p><ul>
<li><b>&ndash;graph</b> (Required). The prefix from which to load the graph data </li>
<li><b>&ndash;format</b> (Required). The format of the input graph. If "weight" is set, the program will read the data file where each line holds [id1] [id2] [weight]. </li>
<li><b>&ndash;partitions</b> (Optional. Default 2). The number of partitions </li>
<li><b>&ndash;svd-dir</b> (Optional. Default ../collaborative_filtering/). Path to the directory where Graphlab svd is located </li>
<li><b>&ndash;kmeans-dir</b> (Optional. Default ../clustering/). Path to the directory where Graphlab kmeans is located </li>
<li><b>&ndash;ncpus</b> (Optional. Default 2). The number of processors that will be used for computation. </li>
<li><b>&ndash;graph_opts</b> (Optional, Default empty). Any additional graph options. See <a class="el" href="classgraphlab_1_1distributed__graph.html" title="A directed graph datastructure which is distributed across multiple machines. ">graphlab::distributed_graph</a> a list of options. </li>
<li><b>&ndash;mpi-args</b> (Optional, Default empty). If set, will execute mipexec with the given string.</li>
</ul>
<h1><a class="anchor" id="graph_analytics_total_subgraph_centrality"></a>
"Total Subgraph Centrality"</h1>
<p>Total subgraph centrality was implemented by Jacob Kesinger, see additional details in his <a href="http://jacobkesinger.tumblr.com/post/64338572799/total-subgraph-centrality">blog post</a>. Total Subgraph Communicability is a new centrality measure due to Benzi&amp;Klymco [1]. For a directed graph with adjacenty matrix A,</p>
<pre class="fragment">TSC_i = sum_j exp(A)_{ij} = (exp(A)*1)_i.
</pre><p>This code calculates the TSC using an Arnoldi iteration on the Krylov subspace {b, Ab,A*Ab, A*A*Ab, ...} due to Saad[1], and using the new warp engine from Graphlab 2.2 (without which this would have been, at best, very challenging).</p>
<p>Small components of large graphs will have bogus answers due to floating point issues. To find the exact TSC for a particular node i, run with "--column i" to find exp(A)*e_i; you will have to sum the resulting output yourself, however.</p>
<p>SAMPLE INPUT: </p><pre class="fragment">0 1
1 2
1 3
2 4
3 4
1 0
2 1
3 1
4 2
4 3
</pre><p>OUTPUT: </p><pre class="fragment">0 5.17784
1 10.3319
2 8.49789
3 8.49789
4 7.96807
</pre><p>You can verify this in python as: </p><pre class="fragment">import scipy
import scipy.linalg
A = scipy.array([[0,1,0,0,0],[1,0,1,1,0],[0,1,0,0,1],[0,1,0,0,1],[0,0,1,1,0]])
scipy.linalg.expm2(A).sum(axis=1)
</pre><p>[1]: Benzi, Michele, and Christine Klymko. Total Communicability as a Centrality Measure. ArXiv e-print, February 27, 2013. <a href="http://arxiv.org/abs/1302.6770">arxiv</a></p>
<p>[2]: Saad, Yousef. “Analysis of Some Krylov Subspace Approximations to the Matrix Exponential Operator.” SIAM Journal on Numerical Analysis 29, no. 1 (1992): 209–228. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="toolkits.html">GraphLab Toolkits</a></li>
    <li class="footer">Generated on Mon Dec 29 2014 17:16:26 for GraphLab: Distributed Graph-Parallel API by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.9 </li>
  </ul>
</div>
</body>
</html>
